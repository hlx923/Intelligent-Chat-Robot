import sys
import os
import streamlit as st
import warnings
import asyncio
import platform
import speech_recognition as sr
import pyttsx3
from PIL import Image
import io
import base64
from datetime import datetime

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['STREAMLIT_SERVER_FILE_WATCHER_TYPE'] = 'none'
os.environ['STREAMLIT_SERVER_ENABLE_STATIC_SERVING'] = 'false'
os.environ['STREAMLIT_SERVER_ENABLE_WEB_SOCKET_CONNECTION'] = 'false'
os.environ['STREAMLIT_SERVER_RUN_ON_SAVE'] = 'true'
os.environ['STREAMLIT_SERVER_ENABLE_CORS'] = 'false'
os.environ['STREAMLIT_SERVER_ENABLE_XSRF_PROTECTION'] = 'false'
os.environ['STREAMLIT_BROWSER_GATHER_USAGE_STATS'] = 'false'

# é¢„å¤„ç†torch.classes
if 'torch.classes' in sys.modules:
    del sys.modules['torch.classes']

class FakePath:
    _path = []

class FakeClasses:
    __path__ = FakePath()

if 'torch' in sys.modules and not hasattr(sys.modules['torch'], 'classes'):
    sys.modules['torch'].classes = FakeClasses()
if 'torch.classes' not in sys.modules:
    sys.modules['torch.classes'] = FakeClasses()

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore", category=RuntimeWarning)

# å¯¼å…¥è‡ªå®šä¹‰å·¥å…·
from utils import generate_response, select_model, detect_language, emotional_response, model_manager

# å¢å¼ºäº‹ä»¶å¾ªç¯å¤„ç†
if platform.system() == "Windows":
    if hasattr(asyncio, 'WindowsProactorEventLoopPolicy'):
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ™ºèƒ½èŠå¤©åŠ©æ‰‹",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
.stApp {
    background-color: #f8f9fa;
}
.chat-message {
    padding: 1.5rem;
    border-radius: 1rem;
    margin-bottom: 1rem;
    box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    max-width: 85%;
}
.user-message {
    background-color: #e3f2fd;
    margin-left: auto;
    border-bottom-right-radius: 0.2rem;
}
.assistant-message {
    background-color: #f3e5f5;
    border-bottom-left-radius: 0.2rem;
}
.system-message {
    background-color: #f5f5f5;
    font-style: italic;
    text-align: center;
    max-width: 100%;
}
.model-status {
    padding: 0.5rem;
    border-radius: 0.5rem;
    margin-bottom: 1rem;
    background-color: #e8f5e9;
}
</style>
""", unsafe_allow_html=True)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'messages' not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append({
        "role": "system",
        "content": "ğŸ‘‹ æ¬¢è¿ä½¿ç”¨æ™ºèƒ½èŠå¤©åŠ©æ‰‹ï¼æˆ‘å¯ä»¥å¸®æ‚¨è§£ç­”é—®é¢˜ã€ç¼–å†™ä»£ç ã€åˆ†æå›¾ç‰‡ç­‰ã€‚",
        "timestamp": datetime.now().strftime("%H:%M:%S")
    })

if 'current_model' not in st.session_state:
    st.session_state.current_model = 'qwen2'

if 'model_params' not in st.session_state:
    st.session_state.model_params = {
        'temperature': 0.7,
        'top_p': 0.9,
        'max_tokens': 2048
    }

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.title("âš™ï¸ ç³»ç»Ÿè®¾ç½®")
    
    # åŸºç¡€è®¾ç½®
    st.subheader("ğŸŒ åŸºç¡€è®¾ç½®")
    language = st.selectbox(
        "ç•Œé¢è¯­è¨€",
        ["ä¸­æ–‡", "English", "æ—¥æœ¬èª", "í•œêµ­ì–´"],
        help="é€‰æ‹©ç•Œé¢æ˜¾ç¤ºè¯­è¨€"
    )
    
    # æ¨¡å‹è®¾ç½®
    st.subheader("ğŸ¯ æ¨¡å‹è®¾ç½®")
    model_mode = st.radio(
        "æ¨¡å‹é€‰æ‹©æ¨¡å¼",
        ["è‡ªåŠ¨é€‰æ‹©", "æ‰‹åŠ¨é€‰æ‹©"],
        help="è‡ªåŠ¨æ¨¡å¼å°†æ ¹æ®è¾“å…¥å†…å®¹æ™ºèƒ½é€‰æ‹©åˆé€‚çš„æ¨¡å‹"
    )
    
    if model_mode == "æ‰‹åŠ¨é€‰æ‹©":
        st.session_state.current_model = st.selectbox(
            "é€‰æ‹©æ¨¡å‹",
            list(model_manager.models.keys()),
            help="é€‰æ‹©è¦ä½¿ç”¨çš„AIæ¨¡å‹"
        )
        
        # æ¨¡å‹å‚æ•°è°ƒæ•´
        st.session_state.model_params['temperature'] = st.slider(
            "æ¸©åº¦ (Temperature)",
            0.0, 1.0, st.session_state.model_params['temperature'],
            help="æ§åˆ¶å›ç­”çš„åˆ›é€ æ€§ï¼Œå€¼è¶Šå¤§å›ç­”è¶Šå¤šæ ·åŒ–"
        )
        st.session_state.model_params['top_p'] = st.slider(
            "é‡‡æ ·é˜ˆå€¼ (Top P)",
            0.0, 1.0, st.session_state.model_params['top_p'],
            help="æ§åˆ¶å›ç­”çš„è´¨é‡ï¼Œå€¼è¶Šå°å›ç­”è¶Šä¿å®ˆ"
        )
    
    # åŠŸèƒ½è®¾ç½®
    st.subheader("ğŸ¨ åŠŸèƒ½è®¾ç½®")
    enable_emotion = st.toggle("å¯ç”¨æƒ…æ„Ÿäº¤äº’", value=True, help="å¼€å¯åAIä¼šç†è§£å¹¶å›åº”æƒ…æ„Ÿ")
    enable_voice = st.toggle("å¯ç”¨è¯­éŸ³äº¤äº’", value=True, help="å¼€å¯åå¯ä»¥ä½¿ç”¨è¯­éŸ³è¾“å…¥å’Œæ’­æŠ¥")
    enable_image = st.toggle("å¯ç”¨å›¾ç‰‡åˆ†æ", value=True, help="å¼€å¯åå¯ä»¥ä¸Šä¼ å›¾ç‰‡è¿›è¡Œåˆ†æ")
    
    # ç³»ç»ŸçŠ¶æ€
    st.subheader("ğŸ“Š ç³»ç»ŸçŠ¶æ€")
    st.info(f"å½“å‰æ¨¡å‹: {st.session_state.current_model}")
    st.info(f"ä¼šè¯æ•°é‡: {len(st.session_state.messages)}")
    
    # æ¸…ç©ºä¼šè¯
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºä¼šè¯è®°å½•"):
        st.session_state.messages = [st.session_state.messages[0]]  # ä¿ç•™ç³»ç»Ÿæ¬¢è¿æ¶ˆæ¯
        st.success("ä¼šè¯å·²æ¸…ç©º")

# ä¸»ç•Œé¢
st.title("ğŸ¤– æ™ºèƒ½èŠå¤©åŠ©æ‰‹")

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(f"{message['content']}")
        st.caption(f"æ—¶é—´: {message.get('timestamp', '')}")

# å›¾ç‰‡ä¸Šä¼ åŠŸèƒ½
if enable_image:
    uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡è¿›è¡Œåˆ†æ", type=["png", "jpg", "jpeg"])
    if uploaded_file is not None:
        image = Image.open(uploaded_file)
        st.image(image, caption="ä¸Šä¼ çš„å›¾ç‰‡", use_column_width=True)
        prompt = f"è¯·åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹å’Œç‰¹ç‚¹"
        
        with st.spinner("æ­£åœ¨åˆ†æå›¾ç‰‡..."):
            response = generate_response(prompt, st.session_state.current_model)
            timestamp = datetime.now().strftime("%H:%M:%S")
            st.session_state.messages.append({
                "role": "assistant",
                "content": response,
                "timestamp": timestamp
            })
            with st.chat_message("assistant"):
                st.markdown(response)
                st.caption(f"æ—¶é—´: {timestamp}")

# è¯­éŸ³è¯†åˆ«
def recognize_speech():
    r = sr.Recognizer()
    max_retries = 3
    
    try:
        with sr.Microphone() as source:
            st.write("è¯·è¯´è¯...")
            st.write("æ­£åœ¨å½•éŸ³...")
            # è°ƒæ•´éº¦å…‹é£å‚æ•°ä»¥æé«˜è¯†åˆ«ç‡
            r.adjust_for_ambient_noise(source, duration=0.5)
            audio = r.listen(source, timeout=5, phrase_time_limit=10)
            st.write("å½•éŸ³å®Œæˆï¼Œæ­£åœ¨è¯†åˆ«...")
        
        # ä¿®æ”¹è¯†åˆ«æœåŠ¡çš„ä¼˜å…ˆçº§ï¼Œä¼˜å…ˆä½¿ç”¨è®¯é£è¯­éŸ³è¯†åˆ«
        from xunfei_speech import recognize_with_xunfei
        try:
            # 1. ä¼˜å…ˆä½¿ç”¨è®¯é£è¯­éŸ³è¯†åˆ«
            result = recognize_with_xunfei(audio.get_raw_data())
            if result:
                return result
        except Exception as e:
            st.warning(f"è®¯é£è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")

        # 2. å¤‡é€‰è¯†åˆ«æœåŠ¡
        recognition_services = [
            lambda: try_google_recognition(r, audio, max_retries),
            lambda: try_sphinx_recognition(r, audio),
            lambda: try_other_recognition(r, audio)
        ]
        
        # ä¾æ¬¡å°è¯•å„ç§è¯†åˆ«æœåŠ¡
        for service in recognition_services:
            try:
                result = service()
                if result:
                    return result
            except Exception as e:
                st.warning(f"è¯†åˆ«æœåŠ¡å°è¯•å¤±è´¥: {str(e)}")
                continue
        
        # å¦‚æœæ‰€æœ‰æœåŠ¡éƒ½å¤±è´¥ï¼Œæä¾›æ‰‹åŠ¨è¾“å…¥é€‰é¡¹
        st.error("æ‰€æœ‰è¯­éŸ³è¯†åˆ«æœåŠ¡å‡ä¸å¯ç”¨")
        manual_input = st.text_input("è¯·æ‰‹åŠ¨è¾“å…¥æ‚¨æƒ³è¯´çš„å†…å®¹:")
        if manual_input:
            return manual_input
        return ""
        
    except Exception as e:
        st.error(f"éº¦å…‹é£è®¿é—®é”™è¯¯: {e}")
        return ""

# Googleè¯­éŸ³è¯†åˆ«ï¼Œå¸¦é‡è¯•æœºåˆ¶
def try_google_recognition(recognizer, audio, max_retries=3):
    retry_count = 0
    while retry_count < max_retries:
        try:
            st.info(f"å°è¯•ä½¿ç”¨Googleè¯­éŸ³è¯†åˆ« (å°è¯• {retry_count+1}/{max_retries})...")
            # ç§»é™¤timeoutå‚æ•°ï¼Œå› ä¸ºrecognize_googleä¸æ”¯æŒæ­¤å‚æ•°
            text = recognizer.recognize_google(audio, language='zh-CN')
            st.success("Googleè¯­éŸ³è¯†åˆ«æˆåŠŸ")
            return text
        except sr.UnknownValueError:
            st.warning("æœªèƒ½è¯†åˆ«è¯­éŸ³å†…å®¹")
            return ""
        except sr.RequestError as e:
            retry_count += 1
            error_msg = str(e)
            st.warning(f"Googleè¯­éŸ³è¯†åˆ«è¯·æ±‚å¤±è´¥ ({retry_count}/{max_retries}): {error_msg}")
            
            # è®°å½•æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            if "Bad Gateway" in error_msg:
                st.error("æ£€æµ‹åˆ°Bad Gatewayé”™è¯¯ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–GoogleæœåŠ¡åœ¨å½“å‰åœ°åŒºä¸å¯ç”¨")
            elif "Connection refused" in error_msg:
                st.error("è¿æ¥è¢«æ‹’ç»ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
            elif "timed out" in error_msg:
                st.error("è¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œé€Ÿåº¦")
                
            if retry_count >= max_retries:
                st.error("Googleè¯­éŸ³è¯†åˆ«æœåŠ¡ä¸å¯ç”¨ï¼Œå°è¯•å…¶ä»–æœåŠ¡")
            import time
            time.sleep(2)  # å¢åŠ ç­‰å¾…æ—¶é—´åˆ°2ç§’
    return ""

# Sphinxç¦»çº¿è¯­éŸ³è¯†åˆ«
def try_sphinx_recognition(recognizer, audio):
    try:
        st.info("å°è¯•ä½¿ç”¨Sphinxç¦»çº¿è¯†åˆ«...")
        # ä¸æŒ‡å®šlanguageå‚æ•°ï¼Œå› ä¸ºé»˜è®¤çš„è‹±æ–‡æ¨¡å‹å¯èƒ½æ¯”ä¸å®Œæ•´çš„ä¸­æ–‡æ¨¡å‹æ•ˆæœæ›´å¥½
        text = recognizer.recognize_sphinx(audio)
        
        if not text:
            st.warning("æœªèƒ½è¯†åˆ«è¯­éŸ³å†…å®¹")
            return ""
            
        st.success("ç¦»çº¿è¯†åˆ«æˆåŠŸ")
        st.info(f"è¯†åˆ«ç»“æœ: '{text}'")
        
        # ä¸å†æ˜¾ç¤ºè­¦å‘Šï¼Œç›´æ¥è¿”å›ç»“æœ
        return text
    except Exception as e:
        st.warning(f"Sphinxç¦»çº¿è¯†åˆ«å¤±è´¥: {e}")
        return ""

# å°è¯•å…¶ä»–å¯ç”¨çš„è¯†åˆ«æœåŠ¡
def try_other_recognition(recognizer, audio):
    # å¯ä»¥æ·»åŠ å…¶ä»–è¯­éŸ³è¯†åˆ«æœåŠ¡ï¼Œå¦‚ç™¾åº¦ã€è®¯é£ç­‰
    # è¿™é‡Œä»…ä½œä¸ºç¤ºä¾‹
    try:
        st.info("å°è¯•ä½¿ç”¨å…¶ä»–è¯­éŸ³è¯†åˆ«æœåŠ¡...")
        # è¿™é‡Œå¯ä»¥æ·»åŠ å…¶ä»–APIçš„è°ƒç”¨
        return ""
    except Exception as e:
        st.warning(f"å…¶ä»–è¯­éŸ³è¯†åˆ«æœåŠ¡å¤±è´¥: {e}")
        return ""

# è¯­éŸ³åˆæˆåŠŸèƒ½
def text_to_speech(text):
    engine = pyttsx3.init()
    engine.setProperty('rate', 150)
    engine.setProperty('volume', 0.9)
    engine.say(text)
    engine.runAndWait()

# è¯­éŸ³è¾“å…¥æŒ‰é’®
if enable_voice:
    if st.button("ğŸ¤ è¯­éŸ³è¾“å…¥"):
        speech_text = recognize_speech()
        if speech_text:
            # ç›´æ¥å¤„ç†è¯†åˆ«ç»“æœï¼Œä¸éœ€è¦é¢å¤–ç¡®è®¤
            timestamp = datetime.now().strftime("%H:%M:%S")
            st.session_state.messages.append({
                "role": "user",
                "content": speech_text,
                "timestamp": timestamp
            })
            with st.chat_message("user"):
                st.markdown(speech_text)
                st.caption(f"æ—¶é—´: {timestamp}")
            
            # ç”Ÿæˆå›ç­”
            with st.spinner("æ€è€ƒä¸­..."):
                if model_mode == "è‡ªåŠ¨é€‰æ‹©":
                    model = select_model(speech_text)
                else:
                    model = st.session_state.current_model
                
                response = generate_response(
                    speech_text,
                    model,
                    **st.session_state.model_params
                )
                
                # æ·»åŠ æƒ…æ„Ÿå›åº”
                if enable_emotion:
                    emotion_response = emotional_response(speech_text)
                    response = f"{emotion_response}\n{response}"
                
                timestamp = datetime.now().strftime("%H:%M:%S")
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": response,
                    "timestamp": timestamp
                })
                with st.chat_message("assistant"):
                    st.markdown(response)
                    st.caption(f"æ—¶é—´: {timestamp}")
                
                # è¯­éŸ³æ’­æ”¾å›ç­”
                text_to_speech(response)

# æ–‡æœ¬è¾“å…¥å¤„ç†
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"):
    timestamp = datetime.now().strftime("%H:%M:%S")
    st.session_state.messages.append({
        "role": "user",
        "content": prompt,
        "timestamp": timestamp
    })
    with st.chat_message("user"):
        st.markdown(prompt)
        st.caption(f"æ—¶é—´: {timestamp}")
    
    # ç”Ÿæˆå›ç­”
    with st.spinner("æ€è€ƒä¸­..."):
        if model_mode == "è‡ªåŠ¨é€‰æ‹©":
            model = select_model(prompt)
        else:
            model = st.session_state.current_model
        
        response = generate_response(
            prompt,
            model,
            **st.session_state.model_params
        )
        
        # æ·»åŠ æƒ…æ„Ÿå›åº”
        if enable_emotion:
            emotion_response = emotional_response(prompt)
            response = f"{emotion_response}\n{response}"
        
        timestamp = datetime.now().strftime("%H:%M:%S")
        st.session_state.messages.append({
            "role": "assistant",
            "content": response,
            "timestamp": timestamp
        })
        with st.chat_message("assistant"):
            st.markdown(response)
            st.caption(f"æ—¶é—´: {timestamp}")
        
        # å¦‚æœå¯ç”¨äº†è¯­éŸ³ï¼Œæ’­æ”¾å›ç­”
        if enable_voice:
            text_to_speech(response)
